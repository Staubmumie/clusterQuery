import os
os.environ["OMP_NUM_THREADS"] = "1"

import matplotlib.pyplot as plt

from collections import Counter
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
import pandas as pd
from sklearn.metrics import silhouette_score
from sklearn.metrics import davies_bouldin_score
import seaborn as sns
import umap.umap_ as umap



def bfs_depth(root):
    """Berechnet die Tiefe eines Baumes mit Breitensuche (BFS)."""
    depth = 0
    queue = [(root, 0)]  # (Knoten, Tiefe)
    visited = set()

    while queue:
        node, current_depth = queue.pop(0)

        if node in visited:
            continue
        visited.add(node)

        # Alle Elternknoten als Knoten f√ºr die n√§chste Ebene hinzuf√ºgen
        for parent, _ in node.parents:
            if parent not in visited:
                queue.append((parent, current_depth + 1))

        depth = max(depth, current_depth)

    return depth

def micro_cluster_analysis(queries, kmeans_labels, data_points, output_dir="micro_results"):
    os.makedirs(output_dir, exist_ok=True)

    # Pr√ºfen, ob TreeNode-Objekte √ºbergeben wurden
    is_tree = hasattr(queries[0], "name") and hasattr(queries[0], "parents")

    # Falls TreeNodes, extrahiere Namen f√ºr Verarbeitung
    if is_tree:
        names = [node.name for node in queries]
    else:
        names = queries

    # Cluster-Abgleich
    clustered_queries = get_clusters(queries if not is_tree else names, kmeans_labels)
    results = []

    for i, (original_label, cluster_names) in enumerate(clustered_queries.items(), start=1):
        cluster_id = i
        
        data_points = np.array(data_points)#rausnehmen falls es zum Typeerror ferhler kommt
        
        # Indizes im Cluster
        indices = [idx for idx, lbl in enumerate(kmeans_labels) if lbl == original_label]
        vectors = data_points[indices]

        centroid = np.mean(vectors, axis=0)
        dists_to_centroid = euclidean_distances(vectors, [centroid])
        rep_idx = np.argmin(dists_to_centroid)
        representative_sentence = cluster_names[rep_idx]
        representative_sentence_safe = f'"{representative_sentence}"'

        # Wortfrequenzen
        token_lists = [s.lower().split() for s in cluster_names]
        all_tokens = [token for tokens in token_lists for token in tokens]
        total_word_freq = Counter(all_tokens)
        word_in_sentence_count = Counter()
        for tokens in token_lists:
            word_in_sentence_count.update(set(tokens))

        def plot_word_freq(counter, title, filename, color):
            if len(counter) == 0:
                return
            top_words = counter.most_common(15)
            words, freqs = zip(*top_words)
            words = [w.replace('$', r'\$') for w in words]
            plt.figure(figsize=(10, 5))
            sns.barplot(x=list(freqs), y=list(words), orient="h", color=color)
            plt.title(title)
            plt.xlabel("Anzahl")
            plt.tight_layout()
            plt.savefig(filename)
            plt.close()

        plot_word_freq(total_word_freq, f"Cluster {cluster_id} ‚Äì Gesamtereignisfrequenz",
                       f"{output_dir}/cluster_{cluster_id}_total_freq.png", "skyblue")
        plot_word_freq(word_in_sentence_count, f"Cluster {cluster_id} ‚Äì Ereignisverteilung √ºber Anfragen",
                       f"{output_dir}/cluster_{cluster_id}_sentence_freq.png", "lightgreen")

        avg_similarity = dists_to_centroid.mean()
        sentence_lengths = [len(tokens) for tokens in token_lists]
        avg_len = np.mean(sentence_lengths)
        std_len = np.std(sentence_lengths)

        result_row = {
            "id": cluster_id,
            "original_label": original_label,
            "anzahl_anfragen": len(cluster_names),
            "repr√§sentant": representative_sentence_safe,
            "intra_koh√§renz": avg_similarity,
            "anfragenl√§nge_avg": avg_len,
            "anfragenl√§nge_std": std_len,
            "anfragen": cluster_names 
        }

        # Falls TreeNodes, Tiefenanalyse hinzuf√ºgen
        if is_tree:
            cluster_nodes = [queries[idx] for idx in indices]
            depths = [bfs_depth(node) for node in cluster_nodes]
            avg_depth = np.mean(depths)
            std_depth = np.std(depths)
        
            result_row["tiefe_avg"] = avg_depth
            result_row["tiefe_std"] = std_depth
            # Tiefenverteilung als Counter
            depth_counter = Counter(depths)
            
            # In CSV-taugliches Format umwandeln: z.B. "2:3, 3:5, 4:2"
            depth_dist_str = ", ".join(f"{depth}:{count}" for depth, count in sorted(depth_counter.items()))
            result_row["tiefe_verteilung"] = depth_dist_str
            # Tiefendiagramm
            plt.figure(figsize=(10, 5))
            sns.histplot(depths, kde=True, color="orchid", bins=15)
            plt.title(f"Tiefenverteilung f√ºr Cluster {cluster_id}")
            plt.xlabel("Tiefe")
            plt.ylabel("Anzahl der Knoten")
            plt.tight_layout()
            plt.savefig(f"{output_dir}/cluster_{cluster_id}_depth_distribution.png")
            plt.close()


        # Satzl√§ngen-Diagramm
        plt.figure(figsize=(10, 5))
        sns.histplot(sentence_lengths, kde=True, color="skyblue", bins=20)
        plt.title(f"Verteilung der Anfragenl√§ngen f√ºr Cluster {cluster_id}")
        plt.xlabel("Anfragel√§nge")
        plt.ylabel("Anzahl der Anfragen")
        plt.tight_layout()
        plt.savefig(f"{output_dir}/cluster_{cluster_id}_sentence_length_distribution.png")
        plt.close()

        results.append(result_row)

    df = pd.DataFrame(results)
    csv_path = f"{output_dir}/mikroauswertung.csv"
    df.to_csv(csv_path, index=False)
    plot_cluster_quality(df, output_dir)
    print(f"[‚úî] Mikro-Auswertung abgeschlossen: {len(results)} Cluster analysiert.")
    print(f"[üìÑ] CSV gespeichert unter: {csv_path}")

def plot_cluster_quality(df, output_dir="micro_results"):
    """
    Erstellt ein Diagramm, das verschiedene Cluster-Qualit√§tsmetriken darstellt.
    
    Parameters:
        df (DataFrame): Die DataFrame, die alle Cluster-Metriken enth√§lt.
        output_dir (str): Das Verzeichnis, in dem die Diagramme gespeichert werden.
    """
    # Clustergr√∂√üe (Anzahl der S√§tze)
    plt.figure(figsize=(10, 5))
    sns.barplot(x=df['id'], y=df['anzahl_anfragen'], color='lightblue')
    plt.title("Clustergr√∂√üe (Anzahl der Anffragen)")
    plt.xlabel("Cluster ID")
    plt.ylabel("Anzahl der Anfragen")
    plt.tight_layout()
    plt.savefig(f"{output_dir}/cluster_size_distribution.png")
    plt.close()

    # Intra-Cluster-Koh√§renz
    plt.figure(figsize=(10, 5))
    sns.barplot(x=df['id'], y=df['intra_koh√§renz'], color='skyblue')
    plt.title("Intra-Cluster Koh√§renz")
    plt.xlabel("Cluster ID")
    plt.ylabel("Intra-Cluster Koh√§renz")
    plt.tight_layout()
    plt.savefig(f"{output_dir}/intra_cluster_coherence.png")
    plt.close()

    # Durchschnittliche Satzl√§nge
    plt.figure(figsize=(10, 5))
    sns.barplot(x=df['id'], y=df['anfragenl√§nge_avg'], color='lightgreen')
    plt.title("Durchschnittliche Anfragel√§nge")
    plt.xlabel("Cluster ID")
    plt.ylabel("Durchschnittliche Anfragel√§nge")
    plt.tight_layout()
    plt.savefig(f"{output_dir}/average_sentence_length.png")
    plt.close()

    # Standardabweichung der Satzl√§ngen
    plt.figure(figsize=(10, 5))
    sns.barplot(x=df['id'], y=df['anfragenl√§nge_std'], color='salmon')
    plt.title("Standardabweichung der Anfragenl√§nge")
    plt.xlabel("Cluster ID")
    plt.ylabel("Standardabweichung der Anfragel√§nge")
    plt.tight_layout()
    plt.savefig(f"{output_dir}/sentence_length_std.png")
    plt.close()

    print("[‚úî] Cluster-Qualit√§ts-Diagramme gespeichert.")


def macro_cluster_clustering(data_points, cluster_labels, output_dir):
    """
    F√ºhrt die Auswertung der KMeans-Clusterung durch, einschlie√ülich Berechnung von Silhouetten-Score,
    Davies-Bouldin-Index, Clustergr√∂√üenverteilung und UMAP-Visualisierung.
    
    Parameters:
        data_points (array): Die Datenpunkte, die geclustert wurden.
        cluster_labels (array): Die Cluster-Zuordnungen der Datenpunkte.
        save_metrics_path (str): Der Pfad, unter dem die Metriken gespeichert werden.
        output_dir (str): Der Ordner, in dem die Diagramme und Ergebnisse gespeichert werden.
    
    Returns:
        tuple: (cluster_labels, sil_score, db_index)
    """
    # Metriken berechnen
    sil_score = silhouette_score(data_points, cluster_labels)
    db_index = davies_bouldin_score(data_points, cluster_labels)
    
    
    
    # Clustergr√∂√üen z√§hlen
    unique_labels, counts = np.unique(cluster_labels, return_counts=True)
    
    # ‚úÖ Metriken speichern
    save_metrics_path = f"{output_dir}/makroauswertung.txt"
    with open(save_metrics_path, "w") as f:
        f.write("Makro-Auswertung der Cluster:\n")
        f.write(f"Anzahl Cluster: {len(unique_labels)}\n")
        f.write(f"Silhouetten-Score: {sil_score:.4f}\n")
        f.write(f"Davies-Bouldin Index: {db_index:.4f}\n")
        f.write("Clustergr√∂√üen:\n")
        for label, count in zip(unique_labels, counts):
            f.write(f"  Cluster {label}: {count} Punkte\n")

    print(f"[‚úî] Metriken gespeichert in '{save_metrics_path}'")

    # üìä Clustergr√∂√üenverteilung plotten
    plt.figure(figsize=(10, 5))
    if len(unique_labels) > 60:  # Wenn es viele Cluster sind, dann als Histogramm
        plt.hist(counts, bins=30, color='skyblue', edgecolor='black')
        plt.xlabel("Clustergr√∂√üe (Anzahl Punkte)")
        plt.ylabel("H√§ufigkeit")
        plt.title("Clustergr√∂√üenverteilung (Histogramm)")
    else:  # Wenn es weniger Cluster gibt, dann als Balkendiagramm
        plt.bar(unique_labels, counts, color='skyblue')
        plt.xlabel("Cluster-Label")
        plt.ylabel("Anzahl der Punkte")
        plt.title("Clustergr√∂√üenverteilung")
        plt.xticks(unique_labels)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/cluster_size_distribution.png")
    plt.close()

    # üìç UMAP-Visualisierung
    reducer = umap.UMAP(random_state=42)
    embedding_2d = reducer.fit_transform(data_points)

    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=embedding_2d[:, 0], y=embedding_2d[:, 1],
                    hue=cluster_labels, palette='tab20', s=50, alpha=0.8)
    plt.title("UMAP-Darstellung der Cluster")
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    # Legende unter den Plot
    plt.legend(
        title="Cluster", bbox_to_anchor=(0.5, -0.3), loc='upper center',
        ncol=6, frameon=False
    )
    plt.tight_layout()
    plt.savefig(f"{output_dir}/umap_cluster_visualization.png")
    plt.close()

    return cluster_labels, sil_score, db_index

def get_clusters(queries, kmeans_labels):
    """
    Gibt die Anfragen zur√ºck, die zu den jeweiligen Clustern geh√∂ren.

    Parameters:
        queries (list): Liste der urspr√ºnglichen Anfragen.
        kmeans_labels (array): Die Labels der KMeans-Cluster (die Clusterzuordnung f√ºr jede Anfrage).

    Returns:
        dict: Ein Dictionary, das jedem Cluster die zugeh√∂rigen Anfragen zuordnet.
    """
    clustered_queries = {}

    # Durchlaufe die KMeans-Labels und ordne die Anfragen ihren Clustern zu
    for i, label in enumerate(kmeans_labels):
        if label not in clustered_queries:
            clustered_queries[label] = []
        clustered_queries[label].append(queries[i])

    return clustered_queries